# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tR285gejkGXNXJg4cjJn4Cb1FSkoVeb7
"""

# --- Credit Risk Model: Correct Version ---
from sklearn.datasets import fetch_openml
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# 1️⃣ Load the dataset
data = fetch_openml("credit-g", version=1, as_frame=True)
df = data.frame  # this is the actual DataFrame

# 2️⃣ Check columns (optional)
print("Columns in dataset:", df.columns.tolist())

# 3️⃣ Prepare target and features
y = df['class'].map({'good': 0, 'bad': 1})  # 'good' -> 0, 'bad' -> 1
X = df.drop('class', axis=1)

# 4️⃣ Convert categorical variables to numeric
X = pd.get_dummies(X, drop_first=True)

# 5️⃣ Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 6️⃣ Train Logistic Regression model
model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)

# 7️⃣ Predict and evaluate
y_pred = model.predict(X_test)
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

# --- Optional: Scale features to improve Logistic Regression ---
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Retrain model on scaled features
model = LogisticRegression(max_iter=500)
model.fit(X_train_scaled, y_train)

# Predict
y_pred = model.predict(X_test_scaled)

# Evaluate
from sklearn.metrics import classification_report, accuracy_score
print("Classification Report (Scaled Features):\n")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# --- SHAP Explainability (Clean + Working) ---
!pip install shap -q
import shap

# Create explainer
explainer = shap.Explainer(model, X_train_scaled)

# Get SHAP values
shap_values = explainer(X_test_scaled)

# Plot
shap.plots.beeswarm(shap_values)

# --- SHAP Explainability ---
!pip install shap -q
import shap

# Create explainer
explainer = shap.Explainer(model, X_train_scaled)

# Compute SHAP values for test set
shap_values = explainer(X_test_scaled)

# Plot feature importance
shap.plots.beeswarm(shap_values)

import matplotlib.pyplot as plt

# Save the beeswarm plot
shap.plots.beeswarm(shap_values, show=False)
plt.savefig("shap_beeswarm.png", dpi=300, bbox_inches='tight')
plt.close()

# Optional: Save bar chart of feature importance
shap.summary_plot(shap_values, X_test_scaled, plot_type="bar", show=False)
plt.savefig("shap_feature_importance.png", dpi=300, bbox_inches='tight')
plt.close()

# >>> Full reproducible cell: load data, train model, compute SHAP, save images & report <<<
# Run this whole block in one cell (fresh runtime recommended)

# Install SHAP if needed
!pip install shap -q

# Imports
from sklearn.datasets import fetch_openml
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
import shap
import matplotlib.pyplot as plt
import joblib
import os

# 1) Load dataset
data = fetch_openml("credit-g", version=1, as_frame=True)
df = data.frame.copy()

# 2) Prepare target and features
y = df['class'].map({'good': 0, 'bad': 1})
X = df.drop('class', axis=1)

# 3) One-hot encode categorical features
X = pd.get_dummies(X, drop_first=True)

# 4) Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5) Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6) Train logistic regression (use solver and higher max_iter to avoid convergence warnings)
model = LogisticRegression(max_iter=3000, solver='saga')
model.fit(X_train_scaled, y_train)

# 7) Predict & evaluate
y_pred = model.predict(X_test_scaled)
report_text = classification_report(y_test, y_pred)
print("Classification Report:\n")
print(report_text)
print("Accuracy:", accuracy_score(y_test, y_pred))

# Save classification report as CSV and text
report_dict = classification_report(y_test, y_pred, output_dict=True)
pd.DataFrame(report_dict).transpose().to_csv("classification_report.csv")
with open("classification_report.txt", "w") as f:
    f.write(report_text + "\nAccuracy: " + str(accuracy_score(y_test, y_pred)))

# 8) SHAP explainability: build explainer and compute values
explainer = shap.Explainer(model, X_train_scaled)   # auto-detects model type
shap_values = explainer(X_test_scaled)

# 9) Save SHAP plots as images
# Beeswarm
shap.plots.beeswarm(shap_values, show=False)
plt.savefig("shap_beeswarm.png", dpi=300, bbox_inches="tight")
plt.close()

# Feature importance bar chart
shap.summary_plot(shap_values, features=X_test, feature_names=X_test.columns, plot_type="bar", show=False)
plt.savefig("shap_feature_importance.png", dpi=300, bbox_inches="tight")
plt.close()

# 10) Save model + scaler (optional)
joblib.dump(model, "logistic_model.joblib")
joblib.dump(scaler, "scaler.joblib")

# 11) List files to confirm
print("\nSaved files in notebook working directory:")
!ls -lah

print("\nDone — files you probably want to download: shap_beeswarm.png, shap_feature_importance.png, classification_report.csv, credit_risk_project.ipynb (if you downloaded it).")
